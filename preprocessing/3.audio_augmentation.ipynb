{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea7ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a95feb",
   "metadata": {},
   "source": [
    "# 1. Room Impulse Response\n",
    "- 대부분의 오디오 데이터셋은 조용한 공간에서 녹음되었습니다.\n",
    "- 그러나 마이크가 어떤 공간에 있느냐에 따라서 입력 오디오의 특징이 조금씩 달라질 수 있습니다.\n",
    "- 아래는 다양한 공간의 음향 환경을 무작위로 만들 수 있는 함수입니다. \n",
    "- 출처는 주석으로 표시하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7fb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SRPOL-AUI/storir\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "650df89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rir\n",
    "# audio_utils\n",
    "\n",
    "def decibels_to_gain(decibels: float):\n",
    "    \"\"\"\n",
    "    Change unit from decibels to gains.\n",
    "    Args:\n",
    "        decibels: value in decibels.\n",
    "\n",
    "    Returns:\n",
    "        value in gains.\n",
    "    \"\"\"\n",
    "    return 10 ** (decibels / 20)\n",
    "\n",
    "\n",
    "def peak_norm(audio: np.ndarray):\n",
    "    \"\"\"\n",
    "    Audio normalisation with respect to highest peak to obtain (-1, 1) amplitudes.\n",
    "    Args:\n",
    "        audio: signal to be normalised\n",
    "\n",
    "    Returns:\n",
    "        normalised signal\n",
    "    \"\"\"\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "# rir_utils\n",
    "\n",
    "def calculate_drr_energy_ratio(y, direct_sound_idx):\n",
    "    \"\"\"\n",
    "    Calculates the direct to reverberant sound energy ratio.\n",
    "    Args:\n",
    "        y: energetic impulse response\n",
    "        direct_sound_idx: index of the initial sound ray\n",
    "\n",
    "    Returns:\n",
    "        drr energy ratio\n",
    "    \"\"\"\n",
    "    # everything up to the given idx is summed up and treated as direct sound energy\n",
    "    direct = sum(y[:direct_sound_idx + 1])\n",
    "    reverberant = sum(y[direct_sound_idx + 1:])\n",
    "    drr = 10 * np.log10(direct / reverberant)\n",
    "    return drr\n",
    "\n",
    "\n",
    "def thin_out_reflections(y, start_idx, end_idx, rate):\n",
    "    \"\"\"\n",
    "    Randomly deletes a fraction of sound rays in a specified time window.\n",
    "    Args:\n",
    "        y: energetic impulse response\n",
    "        start_idx: time window starting sample index\n",
    "        end_idx: time window ending sample index\n",
    "        rate: the fraction of sound rays to delete\n",
    "\n",
    "    Returns:\n",
    "        energetic IR without fraction of sound rays in specified interval\n",
    "    \"\"\"\n",
    "    ray_indices = [idx for idx in range(start_idx, end_idx + 1) if y[idx] != 0]\n",
    "    num_rays = int(len(ray_indices) * rate)\n",
    "    assert num_rays >= 1\n",
    "    random_subset = np.random.choice(ray_indices, num_rays, replace=False)\n",
    "    y[random_subset] = 0\n",
    "    return y\n",
    "\n",
    "# ir\n",
    "\n",
    "class ImpulseResponse:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            rt60: float,\n",
    "            edt: float,\n",
    "            itdg: float,\n",
    "            er_duration: float,\n",
    "            drr: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Energetic stochastic impulse response.\n",
    "        Args:\n",
    "            rt60: reverberation time [ms]\n",
    "            edt: early decay time [ms]\n",
    "            itdg: initial time delay gap [ms]\n",
    "            er_duration: early reflections duration [ms]\n",
    "            drr: direct to reverberant energy ratio [dB]\n",
    "        \"\"\"\n",
    "        self.rt60 = rt60\n",
    "        self.edt = edt\n",
    "        self.itdg = itdg\n",
    "        self.er_duration = er_duration\n",
    "        self.drr = drr\n",
    "        if self.rt60 <= self.edt:\n",
    "            raise ValueError('RT60 needs to be longer than EDT.')\n",
    "\n",
    "    def generate(self, sampling_rate):\n",
    "        energetic = self._get_noise(sampling_rate).astype('float32')\n",
    "        energetic, dsi, ersi, erei = self._get_edt_and_rt60_slope(energetic, sampling_rate)\n",
    "        energetic = self._randomize_reflections(energetic, dsi, ersi, erei, sampling_rate)\n",
    "        return energetic[dsi:]\n",
    "\n",
    "    def _get_noise(self, sampling_rate):\n",
    "        # initialize random noise (10 dB range)\n",
    "        num_samples = self._get_num_samples(self.rt60, sampling_rate)\n",
    "        noise = np.random.random_sample(size=num_samples) * 10 - 5\n",
    "        return noise\n",
    "\n",
    "    def _get_edt_and_rt60_slope(self, y, sampling_rate):\n",
    "        \"\"\"\n",
    "        Shapes a random vector so it has slope specified by EDT and RT60.\n",
    "        \"\"\"\n",
    "\n",
    "        edt_num_samples = self._get_num_samples(self.edt, sampling_rate)\n",
    "        rt60_num_samples = self._get_num_samples(self.rt60, sampling_rate)\n",
    "        er_duration_num_samples = self._get_num_samples(self.er_duration, sampling_rate)\n",
    "\n",
    "        # shape the EDT slope of the IR\n",
    "        y[:edt_num_samples - 1] -= np.arange(0, edt_num_samples - 1)\n",
    "        y[edt_num_samples - 1:] -= (edt_num_samples - 1)  # last sample of EDT\n",
    "        y = y * 10 / edt_num_samples\n",
    "\n",
    "        # shape the RT60 slope of the IR (after EDT)\n",
    "        k = np.arange(edt_num_samples, rt60_num_samples)\n",
    "        y[edt_num_samples:rt60_num_samples] -= (k - (edt_num_samples + 1)) * 50 / rt60_num_samples\n",
    "\n",
    "        y -= max(y)  # change scale to dBFS (0 dB becomes the maximal level)\n",
    "        y = decibels_to_gain(y) ** 2\n",
    "\n",
    "        # assign values to specific time points in the IR\n",
    "        direct_sound_idx = np.argmax(y)\n",
    "\n",
    "        # if any of the parameters like er_duration set in config exceed the length\n",
    "        # of the whole IR than we just treat the last idx of the IR as the start/end point\n",
    "        # (if the parameters are set logically it will never happen)\n",
    "        er_start_idx = min(direct_sound_idx + 1, len(y) - 1)\n",
    "        er_end_idx = min(er_start_idx + er_duration_num_samples, len(y) - 1)\n",
    "        return y, direct_sound_idx, er_start_idx, er_end_idx\n",
    "\n",
    "    def _randomize_reflections(self, y, direct_sound_idx, early_ref_start, early_ref_end, sampling_rate):\n",
    "        \"\"\"\n",
    "        Creates time gaps between incoming sound rays of the energetic impulse response\n",
    "        in a way that the DRR condition is met as closely as possible.\n",
    "        \"\"\"\n",
    "        y = self._create_initial_time_delay_gap(y, direct_sound_idx, sampling_rate)\n",
    "\n",
    "        # create a 1 dB margin for error (we will never hit the exact drr value)\n",
    "        drr_low = self.drr - .5\n",
    "        drr_high = self.drr + .5\n",
    "\n",
    "        current_drr = calculate_drr_energy_ratio(y=y, direct_sound_idx=direct_sound_idx)\n",
    "\n",
    "        if current_drr > drr_high:\n",
    "            return y\n",
    "\n",
    "        while drr_low > current_drr:\n",
    "\n",
    "            # thin out early reflections\n",
    "            y = thin_out_reflections(y=y,\n",
    "                                     start_idx=early_ref_start,\n",
    "                                     end_idx=early_ref_end,\n",
    "                                     rate=1/8)\n",
    "\n",
    "            # thin out reverberation tail\n",
    "            y = thin_out_reflections(y=y,\n",
    "                                     start_idx=early_ref_end,\n",
    "                                     end_idx=len(y) - 1,\n",
    "                                     rate=1 / 10)\n",
    "\n",
    "            previous_drr = current_drr\n",
    "            current_drr = calculate_drr_energy_ratio(y=y, direct_sound_idx=direct_sound_idx)\n",
    "\n",
    "            # if thinning out reflections did not decrease the DRR it means\n",
    "            # that the maximal DRR possible has been reached\n",
    "            if np.isclose(previous_drr, current_drr):\n",
    "                break\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _create_initial_time_delay_gap(self, y, direct_sound_idx, sampling_rate):\n",
    "        \"\"\"\n",
    "        Creates a time gap between the initial sound ray (direct sound), and the rest of the reverberant rays.\n",
    "        \"\"\"\n",
    "        # if itdg exceeds the length of the whole IR than we just\n",
    "        # treat the last idx of the IR as the end point\n",
    "        # (if the parameters are set logically it will never happen)\n",
    "        itdg_num_samples = self._get_num_samples(self.itdg, sampling_rate)\n",
    "        itdg_end_idx = min(direct_sound_idx + 1 + itdg_num_samples, len(y) - 1)\n",
    "        y[direct_sound_idx + 1:itdg_end_idx] = 0\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_num_samples(param, sampling_rate):\n",
    "        return int((param / 1000) * sampling_rate)\n",
    "    \n",
    "    \n",
    "# convolving\n",
    "# 만들어진 RIR과 오디오 파일을 convolving하면 우리가 가지고 있는 데이터셋에 적용할 수 있습니다.\n",
    "# 예를 들어 복도의 RIR 파일을 내 목소리 파일과 convolving하면 마치 내 목소리가 복도에서 녹음된 것과 같은 효과를 낼 수 있습니다.\n",
    "# https://github.com/josephernest/impulseresponse.py\n",
    "\n",
    "import numpy as np\n",
    "from wave import open\n",
    "import soundfile\n",
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "class Wave:\n",
    "    def __init__(self, data, frame_rate):\n",
    "        self.data = normalize(data)\n",
    "        self.frame_rate = frame_rate\n",
    "\n",
    "    def make_spectrum(self):\n",
    "        amplitudes = np.fft.rfft(self.data)\n",
    "        frequencies = np.fft.rfftfreq(len(self.data), 1 / self.frame_rate)\n",
    "\n",
    "        return Spectrum(amplitudes, frequencies, self.frame_rate)\n",
    "\n",
    "    def zero_padding(self, n):\n",
    "        zeros = np.zeros(n)\n",
    "        zeros[:len(self.data)] = self.data\n",
    "\n",
    "        self.data = zeros\n",
    "\n",
    "    def write(self, file):\n",
    "        reader = open(file, 'w')\n",
    "\n",
    "        reader.setnchannels(1)\n",
    "        reader.setsampwidth(2)\n",
    "        reader.setframerate(self.frame_rate)\n",
    "\n",
    "        frames = self.quantize().tostring()\n",
    "        reader.writeframes(frames)\n",
    "\n",
    "        reader.close()\n",
    "\n",
    "    def quantize(self):\n",
    "        if max(self.data) > 1 or min(self.data) < -1:\n",
    "            self.data = normalize(self.data)\n",
    "\n",
    "        return (self.data * 32767).astype(np.int16)\n",
    "\n",
    "\n",
    "class Spectrum:\n",
    "    def __init__(self, amplitudes, frequencies, frame_rate):\n",
    "        self.amplitudes = np.asanyarray(amplitudes)\n",
    "        self.frequencies = np.asanyarray(frequencies)\n",
    "        self.frame_rate = frame_rate\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Spectrum(self.amplitudes * other.amplitudes, self.frequencies, self.frame_rate)\n",
    "\n",
    "    def make_wave(self):\n",
    "        return Wave(np.fft.irfft(self.amplitudes), self.frame_rate)\n",
    "\n",
    "\n",
    "def convert_wav(file):\n",
    "    data, samprate = soundfile.read(file)\n",
    "    soundfile.write(file, data, samprate, subtype='PCM_16')\n",
    "    #soundfile.write(file, data, samprate)\n",
    "\n",
    "def read_wave(file):\n",
    "    reader = open(file)\n",
    "\n",
    "    _, sampwidth, framerate, nframes, _, _ = reader.getparams()\n",
    "    frames = reader.readframes(nframes)\n",
    "\n",
    "    reader.close()\n",
    "\n",
    "    dtypes = {1: np.int8, 2: np.int16, 4: np.int32}\n",
    "\n",
    "    if sampwidth not in dtypes:\n",
    "        raise ValueError('unsupported sample width')\n",
    "\n",
    "    data = np.frombuffer(frames, dtype=dtypes[sampwidth])\n",
    "\n",
    "    num_channels = reader.getnchannels()\n",
    "    if num_channels == 2:\n",
    "        data = data[::2]\n",
    "\n",
    "    return Wave(data, framerate)\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    high, low = abs(max(data)), abs(min(data))\n",
    "    return data / max(high, low)\n",
    "\n",
    "\n",
    "def convolution_reverb(audio_file, ir_file, output_file):\n",
    "    convert_wav(audio_file)\n",
    "    convert_wav(ir_file)\n",
    "\n",
    "    audio = read_wave(audio_file)\n",
    "    ir = read_wave(ir_file)\n",
    "    \n",
    "    if len(audio.data) > len(ir.data):\n",
    "        ir.zero_padding(len(audio.data))\n",
    "\n",
    "    else:\n",
    "        audio.zero_padding(len(ir.data))\n",
    "\n",
    "    ir_spectrum = ir.make_spectrum()\n",
    "    audio_spectrum = audio.make_spectrum()\n",
    "\n",
    "    convolution = audio_spectrum * ir_spectrum\n",
    "    wave = convolution.make_wave()\n",
    "    wave.write(output_file)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a647d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making RIR\n",
    "\n",
    "rt60 = 500 #500\n",
    "edt = 50 #50\n",
    "itdg = 3 #3\n",
    "er_duration = 80 #80\n",
    "drr = int(rt60 * (-1 /100)) + np.random.randint(0, np.ceil(rt60 * (1 / 100)))\n",
    "sr = 16000\n",
    "\n",
    "\"\"\"\n",
    "Energetic stochastic impulse response.\n",
    "Args:\n",
    "    rt60: reverberation time [ms]\n",
    "    edt: early decay time [ms]\n",
    "    itdg: initial time delay gap [ms]\n",
    "    er_duration: early reflections duration [ms]\n",
    "    drr: direct to reverberant energy ratio [dB]\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "rir = ImpulseResponse(rt60=rt60,\n",
    "                     edt=edt,\n",
    "                     itdg=itdg,\n",
    "                     drr=drr,\n",
    "                     er_duration=er_duration)\n",
    "\n",
    "# get n impulse responses\n",
    "n=3000\n",
    "for i in range(n):\n",
    "    try:\n",
    "        output = rir.generate(sampling_rate=sr)\n",
    "        rt60 = np.random.randint(100, 900) #500\n",
    "        edt = np.random.randint(10, 90) #50\n",
    "        itdg = np.random.randint(1, 5) #3\n",
    "        er_duration = np.random.randint(40, 90) #80\n",
    "        drr = int(rt60 * (-1 /100)) + np.random.randint(0, np.ceil(rt60 * (1 / 100)))\n",
    "\n",
    "        #draw_spectrogram(output)\n",
    "        sf.write('./Dataset_audio/RIR/' + str(i) + '_IR.wav', output, 16000)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2561b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8116 8016\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(r'./Dataset_audio/Speaking/')),\n",
    "         len(os.listdir(r'./Dataset_audio/OtherSound/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8953970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIR (Speaking)\n",
    "\n",
    "speaking_path = r'./Dataset_audio/Speaking/'\n",
    "rir_path = r'./Dataset_audio/RIR/'\n",
    "    \n",
    "\n",
    "speaking_files = []\n",
    "for i in os.listdir(speaking_path):\n",
    "    if i.endswith('.wav') | i.endswith('.WAV'):\n",
    "        speaking_files.append(i)\n",
    "\n",
    "        \n",
    "rir_files = os.listdir(rir_path)\n",
    "\n",
    "for i in range(len(speaking_files)):\n",
    "    if speaking_files[i].endswith('.wav') | speaking_files[i].endswith('.WAV'):\n",
    "        # rir sampling\n",
    "        rir_file = random.sample(rir_files, 1)[0]\n",
    "        # convolution audio and rir\n",
    "        #convolution_reverb('audio.wav', 'ir.wav', 'result.wav')\n",
    "        convolution_reverb(speaking_path + speaking_files[i], rir_path + rir_file, speaking_path + 'RIR/' + speaking_files[i][:-4]+'_rir.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0c341b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIR(OtherSound)\n",
    "\n",
    "OtherSound_path = r'./Dataset_audio/OtherSound/'\n",
    "rir_path = r'./Dataset_audio/RIR/'\n",
    "\n",
    "OtherSound_files = []\n",
    "for i in os.listdir(OtherSound_path):\n",
    "    if i.endswith('.wav') | i.endswith('.WAV'):\n",
    "        OtherSound_files.append(i)\n",
    "\n",
    "rir_files = os.listdir(rir_path)\n",
    "        \n",
    "for i in range(len(OtherSound_files)):\n",
    "    if OtherSound_files[i].endswith('.wav') | OtherSound_files[i].endswith('.WAV'):\n",
    "        # rir sampling\n",
    "        rir_file = random.sample(rir_files, 1)[0]\n",
    "        # convolution audio and rir\n",
    "        #convolution_reverb('audio.wav', 'ir.wav', 'result.wav')\n",
    "        convolution_reverb(OtherSound_path + OtherSound_files[i], rir_path + rir_file, OtherSound_path + 'RIR/' + OtherSound_files[i][:-4]+'_rir.wav')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aec05",
   "metadata": {},
   "source": [
    "## time streching\n",
    "- 소리를 빠르게 하거나 느리게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7758ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time-stretching the wave (Speaking)\n",
    "'''\n",
    "Permissible factor values = 0 < x < 1.0\n",
    "'''\n",
    "# wav_time_stch = librosa.effects.time_stretch(wav,factor)\n",
    "\n",
    "\n",
    "path = r'./Dataset_audio/Speaking/'\n",
    "files = os.listdir(path)\n",
    "selected = random.sample(files, int(len(files) * 0.50))\n",
    "factor = [0.8, 0.9, 1.1, 1.2]\n",
    "random_speed = random.sample(factor, 1)[0]\n",
    "\n",
    "for file in selected:\n",
    "    if file.endswith('.wav'):\n",
    "        random_speed = random.sample(factor, 1)[0]\n",
    "        data, sr = librosa.load(path + file)\n",
    "        speed_data = librosa.effects.time_stretch(data,random_speed)\n",
    "        speed_path = path + 'SpeedShifted/' + file[:-4] + '_SpeedShifted.wav'\n",
    "        sf.write(speed_path, speed_data, sr, format='wav')\n",
    "        \n",
    "        random_speed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d91e6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time-stretching the wave (OtherSound)\n",
    "'''\n",
    "Permissible factor values = 0 < x < 1.0\n",
    "'''\n",
    "# wav_time_stch = librosa.effects.time_stretch(wav,factor)\n",
    "\n",
    "path = r'./Dataset_audio/OtherSound/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "selected = random.sample(files, int(len(files) * 0.5))\n",
    "factor = [0.8, 0.9, 1.1, 1.2]\n",
    "random_speed = random.sample(factor, 1)[0]\n",
    "\n",
    "for file in selected:\n",
    "    if file.endswith('.wav'):\n",
    "        random_speed = random.sample(factor, 1)[0]\n",
    "        data, sr = librosa.load(path + file)\n",
    "        speed_data = librosa.effects.time_stretch(data,random_speed)\n",
    "        speed_path = path + 'SpeedShifted/' + file[:-4] + '_SpeedShifted.wav'\n",
    "        sf.write(speed_path, speed_data, sr, format='wav')\n",
    "        \n",
    "        random_speed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff99312",
   "metadata": {},
   "source": [
    "## pitch shifting\n",
    "- 음의 높낮이 조절하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e03ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch shifting (Speaking)\n",
    "\n",
    "path = r'./Dataset_audio/Speaking/'\n",
    "files = os.listdir(path)\n",
    "selected = random.sample(files, int(len(files) * 0.50))\n",
    "\n",
    "factor = [-2, -1, 1, 2]\n",
    "random_pitch = random.sample(factor, 1)[0]\n",
    "\n",
    "for file in selected:\n",
    "    if file.endswith('.wav'):\n",
    "        random_pitch = random.sample(factor, 1)[0]\n",
    "        data, sr = librosa.load(path+file)\n",
    "        pitch_data = librosa.effects.pitch_shift(data,sr,n_steps=random_pitch)\n",
    "        pitch_path = path + 'PitchShifted/' + file[:-4] + '_PitchShifted.wav'\n",
    "        sf.write(pitch_path, pitch_data, sr, format='wav')\n",
    "        \n",
    "        random_pitch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f65d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch shifting (OtherSound)\n",
    "\n",
    "path = r'./Dataset_audio/OtherSound/'\n",
    "files = os.listdir(path)\n",
    "selected = random.sample(files, int(len(files) * 0.5))\n",
    "\n",
    "factor = [-2, -1, 1, 2]\n",
    "random_pitch = random.sample(factor, 1)[0]\n",
    "\n",
    "for file in selected:\n",
    "    if file.endswith('.wav'):\n",
    "        random_pitch = random.sample(factor, 1)[0]\n",
    "        data, sr = librosa.load(path+file)\n",
    "        pitch_data = librosa.effects.pitch_shift(data,sr,n_steps=random_pitch)\n",
    "        pitch_path = path + 'PitchShifted/' + file[:-4] + '_PitchShifted.wav'\n",
    "        sf.write(pitch_path, pitch_data, sr, format='wav')\n",
    "        \n",
    "        random_pitch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea5a0d",
   "metadata": {},
   "source": [
    "## 진폭 줄이기\n",
    "- 학습 데이터의 진폭을 소량 줄임으로써 작은 소리에도 강인하도록 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad6b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dB (Speaking)\n",
    "speaking_path = r'./Dataset_audio/Speaking/sliced/'\n",
    "reduced_path = r'./Dataset_audio/Speaking/Reduced/'\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def reducing_dB(path, reduced_path, file, reduce_factor):\n",
    "    sound = AudioSegment.from_wav(os.path.join(path,file))\n",
    "    reduced = sound - reduce_factor # reduce volume by 'reduce_factor' dB\n",
    "    \n",
    "    reduced.export(reduced_path + file + '_reduced' + '.wav', 'wav')\n",
    "    \n",
    "factor = [x for x in range(5,15)]\n",
    "random_factor = random.sample(factor, 1)[0]\n",
    "\n",
    "speaking_files = os.listdir(speaking_path)\n",
    "#speaking_selected = random.sample(speaking_files, int(len(speaking_files) * 1.0))\n",
    "\n",
    "\n",
    "for file in speaking_files:\n",
    "    if file.endswith('.wav') | file.endswith('.WAV'):\n",
    "        random_factor = random.sample(factor, 1)[0]\n",
    "        reducing_dB(speaking_path, reduced_path, file, random_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08666eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dB (OtherSound)\n",
    "OtherSound_path = r'./Dataset_audio/OtherSound/sliced/'\n",
    "reduced_path = r'./Dataset_audio/OtherSound/Reduced/'\n",
    "\n",
    "OtherSound_files = os.listdir(OtherSound_path)\n",
    "\n",
    "factor = [x for x in range(5,15)]\n",
    "random_factor = random.sample(factor, 1)[0]\n",
    "\n",
    "for file in OtherSound_files:\n",
    "    if file.endswith('.wav') | file.endswith('.WAV'):\n",
    "        random_factor = random.sample(factor, 1)[0]\n",
    "        reducing_dB(OtherSound_path, reduced_path, file, random_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.5",
   "language": "python",
   "name": "venv3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
